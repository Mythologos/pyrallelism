from typing import Any, Optional

from numpy.typing import NDArray

from pyrallelism.primitives.assignment.lsa import LinearSumAssigner
from pyrallelism.primitives.evaluation_metric import EvaluationMetric
from pyrallelism.primitives.typing import LSAComponents, ParallelismDirectory
from pyrallelism.structures.confusion_matrix import ReducedConfusionMatrix


def evaluate_bipartite_parallelism_metric(hypotheses: ParallelismDirectory, references: ParallelismDirectory,
                                          metric: EvaluationMetric,
                                          scoring_kwargs: Optional[dict[str, Any]] = None,
                                          size_kwargs: Optional[dict[str, Any]] = None) -> \
        tuple[ReducedConfusionMatrix, LSAComponents]:
    """
    A function which mediates the process of computing central values for the family of bipartite parallelism metrics.
    :param hypotheses: a collection of hypothesized parallelisms in the form of a `ParallelismDirectory` object.
    :param references: a collection of ground truth parallelisms in the form of a `ParallelismDirectory` object.
    :param metric: an `EvaluationMetric` containing a coordinated combination of
    a `ScoringFunction` class and a `SizeFunction` class.
    :param scoring_kwargs: a collection of keyword arguments meant to modify the scoring calculations.
    :param size_kwargs: a collection of keyword arguments meant to modify the size calculations.
    :return: a 2-tuple of values, including: (1) `new_confusion_matrix`, the overall matching score obtained through
    the bipartite maximal matching algorithm and the two total sizes derived from supplied parallelism directories;
    (2) `computation_components`, a `dict` containing steps of the bipartite parallelism metric computation:
    an `NDArray` filled with matching scores generated by `scoring_function` from `hypotheses` and `references`, and
    a `list` of coordinates to that matrix which pertain to the maximum matching generated  by the LSA algorithm.
    """
    scoring_kwargs = {} if scoring_kwargs is None else scoring_kwargs
    size_kwargs = {} if size_kwargs is None else size_kwargs

    scoring_matrix: NDArray[int] = metric.score.create_score_matrix(hypotheses, references, **scoring_kwargs)
    entries: list[tuple[int, int]] = LinearSumAssigner.get_lsa_entries(scoring_matrix)
    computation_components: LSAComponents = {"scoring_matrix": scoring_matrix, "entries": entries}

    new_confusion_matrix: ReducedConfusionMatrix = ReducedConfusionMatrix()
    new_confusion_matrix.score = LinearSumAssigner.get_lsa_score(scoring_matrix, entries)
    new_confusion_matrix.hypothesis_count = metric.size.compute_directory_size(hypotheses, **size_kwargs)
    new_confusion_matrix.reference_count = metric.size.compute_directory_size(references, **size_kwargs)

    return new_confusion_matrix, computation_components
